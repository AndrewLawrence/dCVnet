% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dCVnet_main.R
\name{dCVnet}
\alias{dCVnet}
\title{Fit a doubly cross-validated elastic-net regularised logistic regression}
\usage{
dCVnet(f, data, nrep_outer = 2, k_outer = 10, nrep_inner = 5,
  k_inner = 10, alphalist = c(0.2, 0.5, 0.8), nlambda = 100,
  type.measure = "deviance", positive = 1,
  opt.lambda.type = c("minimum", "se", "percentage"),
  opt.lambda.type.value = 1, opt.empirical_cutoff = FALSE,
  opt.uniquefolds = FALSE, opt.ystratify = TRUE, ...)
}
\arguments{
\item{f}{a two sided formula.
The LHS must be a single binary variable.
The RHS must refer to columns in \code{data}, but may include
interactions, transformations or expansions (like \code{\link{poly}}, or
\code{\link{log}}).
The formula \emph{must} include an intercept.}

\item{data}{a data.frame containing all terms in f.}

\item{nrep_outer}{an integer, the number of repetitions (k-fold outer CV)}

\item{k_outer}{an integer, the k in the outer k-fold CV.}

\item{nrep_inner}{an integer, the number of repetitions (k-fold inner CV)}

\item{k_inner}{an integer, the k in the inner k-fold CV.}

\item{alphalist}{a numeric vector of values in (0,1].
This sets the search space for optimising hyperparameter alpha.}

\item{nlambda}{an integer, number of gradations between
lambda.min and lambda.max to search.
See \code{glmnet} argument \code{nlambda}.}

\item{type.measure}{passed to \code{\link[glmnet]{cv.glmnet}}.
This sets the metric used for hyperparameter optimisation in the
inner cross-validation. Options: \code{"deviance"}, \code{"class"},
\code{"mse"}, \code{"mae"}}

\item{positive}{What level of the outcome is a 'positive' result
(in the sense of a diagnostic test).
Can be a numeric indicating which of \code{levels(y)} to use
(i.e. 1 | 2). Alternatively a character specifying the exact level
(e.g. \code{"patient"}).}

\item{opt.lambda.type}{Method for selecting optimum lambda. One of
\itemize{
\item{\code{"minimum"} - returns the lambda with best
CV score.}
\item{\code{"se"} - returns the +1 se lambda}
\item{\code{"percentage"} - returns minimum lambda
scaled by a factor, e.g. allowing lambda+3pc}
}}

\item{opt.lambda.type.value}{determines the se multiplier or percentage
for \code{opt.lambda.type}.}

\item{opt.empirical_cutoff}{Boolian.
Use the empirical proportion of cases as the cutoff for outer CV
classification (affects outer CV performance only).
Otherwise classify at 50\% probability.}

\item{opt.uniquefolds}{Boolian.
In most circumstances folds will be unique. This requests
that random folds are checked for uniqueness in inner and outer loops.
Currently it warns if non-unqiue values are found.}

\item{opt.ystratify}{Boolian.
Outer and inner sampling is stratified by outcome.
This is implemented with \code{\link[caret]{createFolds}}}

\item{...}{Arguments to pass through to cv.glmnet
(beware, making use of this may break things).}
}
\value{
a dCVnet object.
}
\description{
Double (nested) repeated k-fold cross-validation used to:
\itemize{
\item{Produce unbiased estimates of out-of-sample
classification performance (outer CV).}
\item{Select optimal hyperparameters for the elasticnet (inner CV).}}
Elasticnet hyperparameters are
\bold{lambda} (the total regularisation penalty)
and \bold{alpha} (the balance of L1 and L2 regularisation types).
}
\examples{
\dontrun{
iris_class <- dCVnet(f = Species ~ Sepal.Length + Sepal.Width +
                         Petal.Length + Petal.Width,
                     data = subset(iris, iris$Species != "versicolor"),
                     alphalist = 0.5)
#Note: in most circumstances larger values of nrep_inner and nrep_outer
#      will be required.
summary(classperformance(iris_class))
plot(iris_class)
}
}
