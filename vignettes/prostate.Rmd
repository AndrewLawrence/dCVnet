---
title: "A dCVnet example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A dCVnet example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this example we will explore an application of dCVnet in a tutorial dataset
provided with the Elements of Statistical Learning (ESL) textbook.

First, get required libraries, load the data and take a look:

```{r setup}
library(tidyverse)
ggplot2::theme_set(theme_minimal())
library(dCVnet)
data(prostate)
psych::describe(prostate[, -10], fast = TRUE) %>%
  knitr::kable(digits = 2)
```

For consistency with examples in ESL we wish to standardise the data:

```{r standardise}
# see ?prostate for more details:
sprostate <- data.frame(scale(prostate[, -10]))
```

Note that we will ignore the train-test split as this vignette will demonstrate cross-validation.

Next noting that the base unregularised model is not very overfit, we can make the problem harder
by adding noise predictors unrelated to the outcome:

```{r add noise}
set.seed(1)
noise <- matrix(rnorm(nrow(sprostate) * 10), nrow = nrow(sprostate))
colnames(noise) <- paste0("noise", 1:10)
sprostate <- cbind(sprostate,
                   noise)

cor(cbind(sprostate$lpsa, noise))[-1, 1]

```

Before cross-validating our estimates of prediction model performance, first lets build some prediction models without cross-validation:

```{r predict_lm, echo=TRUE}
# A ordinary linear regression
m1 <- lm(lpsa ~ ., data = sprostate, y = TRUE, x = TRUE)
```

```{r predict_cv.glmnet}
# a LASSO regularised regression
m2 <- glmnet::cv.glmnet(y = sprostate$lpsa, x = as.matrix(sprostate[, -9]))
```


The resulting coefficients look like this:

```{r coefficients, echo=FALSE}
data.frame(broom::tidy(m1)[, 1:2] %>% setNames(c("Term", "lm")),
           `glmnet\nlambda.min` =
             c(tidy_coef.glmnet(m2, s = "lambda.min")$Coef),
           `glmnet\nlambda.1se` =
             c(tidy_coef.glmnet(m2, s = "lambda.1se")$Coef)) %>%
  knitr::kable(digits = 3)
```


The performance of these models (in the data used to train them) is:

```{r accuracy}

predictdf <- data.frame(
  observed = sprostate$lpsa,
  lm = predict(m1),
  glmnet.lambda.min = c(predict(
    m2,
    s = "lambda.min",
    newx = as.matrix(sprostate[, -9])
  )),
  glmnet.lambda.1se = c(predict(
    m2,
    s = "lambda.1se",
    newx = as.matrix(sprostate[, -9])
  ))
)

sapply(predictdf[, -1], caret::postResample, predictdf[, 1]) %>%
  knitr::kable(digits = 3)

```

Plotting out the predictions against the true values gives:

```{r plot_perf, echo=FALSE, fig.height=7, fig.width=7, message=FALSE, warning=FALSE}

predictdf %>%
  pivot_longer(cols = c("lm", "glmnet.lambda.min", "glmnet.lambda.1se"),
               values_to = "Prediction",
               names_to = "Model") %>%
  ggplot(aes(y = observed, x = Prediction)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  coord_equal() +
  facet_grid(cols = vars(Model), as.table = FALSE)

```


However, these performance results are not cross-validated, we can use dCVnet to produce
cross-validated performance results and improve the tuning of the glmnet models by 

  1) repeating the k-fold CV to stabilise tuning parameter selection
  1) tuning over alpha (elastic-net hyperparameter for the balance between L1 and L2 penalties)
 
First though, we shall produce cross-validated estimates of the lm performance:
```{r cv_lm}
if ( require(lmvar, quietly = TRUE) ) {
  set.seed(42)
  m1.cv <- lmvar::cv.lm(m1, k = 10, max_cores = 1)
  m1.cv.repeat <- lmvar::cv.lm(m1, k = 10, max_cores = 1)
  rbind(repeat1 = data.frame(unclass(m1.cv)),
        repeat2 = data.frame(unclass(m1.cv.repeat))) %>%
      select(-contains(".sd")) %>%
      knitr::kable(digits = 3)
  perf.train <- signif(mean(abs(m1$fitted.values - m1$y)), 3)
  perf.test <- signif(m1.cv$MAE$mean, 3)
} else {
  print("install.packages('lmvar') to run this chunk.")
  perf.train <- signif(mean(abs(m1$fitted.values - m1$y)), 3)
  perf.test <- "LMVAR missing"
}

```

So there is a performance gap for the simple linear model (e.g. in MAE) such
that the complete data performance
(`r signif(perf.train,3)`)
is better than the cross-validated performance
 (`r signif(perf.test,3)`).

We can run the equivalent model in dCVnet:

```{r dCVnet, echo=FALSE}
m3 <- dCVnet(y = sprostate$lpsa,
             data = sprostate[, -9:-10],
             f = "~.",
             family = "gaussian",
             alphalist = 1.0,
             k_inner = 10,
             k_outer = 10,
             nrep_inner = 3,
             nrep_outer = 3)
```

```{r dCVnet_summary }
report_performance_summary(m3)[1:2, ] %>%
  knitr::kable(digits = 3)
```

Taken together we can see that the cross-validated MAE of the LASSO (`r signif(report_performance_summary(m3)[2,2],3)`) is lower/better than the cross-validated MAE of the un-regularised model (`r perf.test`). A comparison between the prediction performances without cross-validation would have concluded that LASSO was less performant (MAE = `r signif(caret::postResample(predictdf$glmnet.lambda.min, predictdf$observed)[["MAE"]],3)`) than an unregularised model (MAE = `r signif(caret::postResample(predictdf$lm, predictdf$observed)[["MAE"]],3)`)

In this example the difference is not dramatic without adding the noise predictors (without noise there's not a lot of overfitting in the unregularised model). So albeit slightly artificially, the principle that tuned regularisation can produce better generalisation error has been demonstrated.


